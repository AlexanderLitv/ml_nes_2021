{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4647c0d",
   "metadata": {},
   "source": [
    "# Семинар 12. Свертки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16034d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import cm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae122a",
   "metadata": {},
   "source": [
    "Загрузим квокку!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O quokka.jpg https://www.meme-arsenal.com/memes/5851dce2a0718490da96a3819221a01a.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9092849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Pillow\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"quokka.jpg\")\n",
    "print(f\"Image format: {img.format}; shape: {img.size}; color scheme: {img.mode}\")\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c178c672",
   "metadata": {},
   "source": [
    "Мы знаем, что цветное изображение состоит из 3 числовых матриц или трехмерного тензора. Каждая матрица соответствует одному из 3 базовых цветов: красному, зеленому и синему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85048c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем тензор\n",
    "img_matrix = np.array(img)\n",
    "\n",
    "# (высота, ширина, число каналов)\n",
    "print(f\"Image matrix shape: {img_matrix.shape}\")\n",
    "\n",
    "plt.imshow(img_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e508e586",
   "metadata": {},
   "source": [
    "## Операция свертки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82561944",
   "metadata": {},
   "source": [
    "В PyTorch свёрточный слой представлен в модуле `nn` функцией [`Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n",
    "\n",
    "Нам пригодится знать про эти параметры: \n",
    "\n",
    "- количество входных каналов `in_channels`\n",
    "- количество выходных каналов `out_channels`\n",
    "- размер ядра `kernel_size`\n",
    "- шаг `stride`\n",
    "- паддинг `padding`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a2a04",
   "metadata": {},
   "source": [
    "**Размер ядра** - `int`, если ядро квадратное и кортеж из двух чисел, если ядро прямоугольное. Задает размер фильтра, с которым производится свертка изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9866691",
   "metadata": {},
   "source": [
    "![no_padding_no_strides.gif](no_padding_no_strides.gif)\n",
    "\n",
    "Эта и следующие анимации взяты [здесь](https://github.com/vdumoulin/conv_arithmetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfb6535",
   "metadata": {},
   "source": [
    "**Шаг** - задает шаг, в пикселях, на который сдвигается фильтр. `int`, если по горизонтали и вертикали сдвигается на одно и то же число. Кортеж из двух чисел, если сдвиги разные."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f219a",
   "metadata": {},
   "source": [
    "![no_padding_strides.gif](no_padding_strides.gif)\n",
    "\n",
    "Шаг: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafe52b",
   "metadata": {},
   "source": [
    "**Паддинг** - количество пикселей, которыми дополняется изображение. Аналогично шагу и размеру ядра, может быть, как `int`, так и кортежем из двух чисел."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c89e3",
   "metadata": {},
   "source": [
    "**Half pading**\n",
    "![same_padding_no_strides.gif](same_padding_no_strides.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3c8e36",
   "metadata": {},
   "source": [
    "### Свертка изображения\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8629f2a",
   "metadata": {},
   "source": [
    "Применим оператор Собеля для детектирования границ на изображении."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fabdc6d",
   "metadata": {},
   "source": [
    "Откуда взялись эти цифры и почему с ними подсвечиваются границы, можно посмотреть [здесь](https://nrsyed.com/2018/02/18/edge-detection-in-images-how-to-derive-the-sobel-operator/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfd2360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf4bd6a",
   "metadata": {},
   "source": [
    "Конвертируем изображение в нужный формат для PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68fc516",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = torch.tensor([img_matrix], dtype=torch.float)\n",
    "img_tensor.size()  # (число изображений, высота, ширина, число каналов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = img_tensor.permute(0, 3, 1, 2)\n",
    "img_tensor.size()  # (число изображений, число каналов, высота, ширина)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632765e3",
   "metadata": {},
   "source": [
    "Зададим оператор Собеля для детектирования горизонтальных границ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_hor = [[-1, -2, -1], \n",
    "             [ 0,  0,  0], \n",
    "             [ 1,  2,  1]]\n",
    "\n",
    "# одна матрица на каждый канал картинки\n",
    "kernels  = [[sobel_hor, sobel_hor, sobel_hor]]\n",
    "kernels = torch.tensor(kernels, dtype=torch.float)\n",
    "kernels.size() # (число выходных каналов, число входных каналов, высота, ширина)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# свернём картинку с подготовленным ядром свёртки\n",
    "img_conv_hor = conv2d(img_tensor, kernels)\n",
    "img_conv_hor = img_conv_hor.permute(0, 2, 3, 1)\n",
    "img_conv_hor.size()  # (число изображений, высота, ширина, число каналов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(1.5 * 7, 1.5 * 4))\n",
    "plt.imshow(torch.abs(img_conv_hor[0, :, :, 0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56268f2",
   "metadata": {},
   "source": [
    "Зададим оператор Собеля для детектирования вертикальных границ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f300fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_ver = [[-1, 0, 1], \n",
    "             [-2, 0, 2], \n",
    "             [-1, 0, 1]]\n",
    "\n",
    "# одна матрица на каждый канал картинки\n",
    "kernel  = [[sobel_ver, sobel_ver, sobel_ver]]\n",
    "kernel = torch.tensor(kernel, dtype=torch.float)\n",
    "kernel.size() #(число выходных каналов, число входных каналов, высота, ширина)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b394bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_conv_ver = conv2d(img_tensor, kernel)\n",
    "\n",
    "img_conv_ver = img_conv_ver.permute(0, 2, 3, 1)\n",
    "img_conv_ver.size() #(число изображений, высота, ширина, число каналов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cce1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(1.5 * 7, 1.5 * 4))\n",
    "plt.imshow(torch.abs(img_conv_ver[0, :, :, 0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a1a23",
   "metadata": {},
   "source": [
    "Объединим два изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea95cb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_conv = torch.sqrt(img_conv_ver**2 + img_conv_hor**2)\n",
    "\n",
    "plt.figure(figsize=(1.5 * 7, 1.5 * 4))\n",
    "plt.imshow(img_conv[0, :, :, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fbf23f",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "По аналогии с примером выше, сверните изображение со случайным ядром такого же размера.\n",
    "\n",
    "**Подсказка:** используйте `torch.rand()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa710eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "\n",
    "# свертка изображения\n",
    "img_conv_ver = conv2d(img_tensor, kernels_random)\n",
    "img_conv_ver = img_conv_ver.permute(0, 2, 3, 1)\n",
    "\n",
    "# рисуем результат\n",
    "plt.figure(figsize=(1.5 * 7, 1.5 * 4))\n",
    "plt.imshow(img_conv_ver[0, :, :, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c2ca15",
   "metadata": {},
   "source": [
    "### Полносвязная нейронная сеть\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c4e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f1b19",
   "metadata": {},
   "source": [
    "Будем работать с датасетом [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html), в котором содержатся изображения 10 классов размером 32 на 32 пикселя с тремя каналами. В тренировочной выборке 50000 изображений, а в тестовой 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./cifar\", \n",
    "    train=True,                             \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "valset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./cifar\", \n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    trainset, \n",
    "    batch_size=16,\n",
    "    shuffle=True, \n",
    "    num_workers=1\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    valset, \n",
    "    batch_size=16,\n",
    "    shuffle=False, \n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c3633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9a6367",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eae5d6",
   "metadata": {},
   "source": [
    "Обучаем полносвязную нейронную сеть для классификации изображений. Она будет работать долго и не классно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3940e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Flatten(),             \n",
    "    nn.Linear(32 * 32 * 3, 512),  \n",
    "    nn.ReLU(),  \n",
    "    nn.Linear(512, 128), \n",
    "    nn.ReLU(),  \n",
    "    nn.Linear(128, 10),       \n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # создаем оптимизатор и передаем туда параметры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5f3baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, n_epochs=5):\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # тренировка\n",
    "        pbar = tqdm(train_dataloader)\n",
    "        for x_train, y_train in pbar:\n",
    "            y_pred = model(x_train)\n",
    "            loss = F.cross_entropy(y_pred, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            pbar.set_description(f'Loss {loss.data}')\n",
    "\n",
    "        # валидация\n",
    "        if epoch % 2 == 0:\n",
    "            val_loss = []\n",
    "            val_accuracy = []\n",
    "            with torch.no_grad():\n",
    "                for x_val, y_val in tqdm(val_dataloader):\n",
    "                    y_pred = model(x_val)\n",
    "                    loss = F.cross_entropy(y_pred, y_val)\n",
    "                    val_loss.append(loss.numpy())\n",
    "                    val_accuracy.extend((torch.argmax(y_pred, dim=-1) == y_val))\n",
    "                \n",
    "            # печатаем метрики\n",
    "            print(f\"Epoch: {epoch}, loss: {np.mean(val_loss)}, accuracy: {np.mean(val_accuracy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faadbe5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5306df",
   "metadata": {},
   "source": [
    "### Сверточный слой"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e348f940",
   "metadata": {},
   "source": [
    "Добавим в нашу сеть сверточный слой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f59628",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=10, kernel_size=5), \n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=4),  \n",
    "    nn.Flatten(),                 \n",
    "    nn.Linear(7 * 7 * 10, 128),       \n",
    "    nn.ReLU(),                    \n",
    "    nn.Linear(128, 10)        \n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8456e154",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b71e3f",
   "metadata": {},
   "source": [
    "### Задание 2 (домашка)\n",
    "\n",
    "По аналогии с предыдущим примером, обучите нейронную сеть, у которой следущие слои:\n",
    "\n",
    "- Сверточный слой с 10 ядрами размером 5\n",
    "- Функция активации ReLU\n",
    "- Уменьшить картинку в 2 раза (по каждому измерению)\n",
    "- Сверточный слой с 20 ядрами размером 5\n",
    "- Функция активации ReLU\n",
    "- Уменьшить картинку в 2 раза (по каждому измерению)\n",
    "- Полносвязный слой со 128 нейронами\n",
    "- Функция активации ReLU\n",
    "- Выходной слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff943752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f7b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4c590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f850ed4e",
   "metadata": {},
   "source": [
    "### Наконец напишем нормальный класс для обучения \n",
    "\n",
    "прям как мы любим)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55128e03",
   "metadata": {},
   "source": [
    "Логировать будем в wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7fc76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module,\n",
    "                 optimizer,\n",
    "                 criterion, \n",
    "                 train_dataset: Dataset,\n",
    "                 val_dataset: Dataset,\n",
    "                 batch_size: int = 16,\n",
    "                 log_wandb: bool = True):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.log_wandb = log_wandb\n",
    "\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "\n",
    "    def save_checkpoint(self, path, epoch, loss):\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),  \n",
    "            'loss': loss\n",
    "        }, path)\n",
    "        \n",
    "\n",
    "    def train(self, num_epochs: int):\n",
    "        model = self.model\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        train_loader = DataLoader(self.train_dataset, shuffle=True, pin_memory=True, batch_size=self.batch_size)\n",
    "        val_loader = DataLoader(self.val_dataset, shuffle=False, pin_memory=True, batch_size=self.batch_size)\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_bar = tqdm(train_loader)\n",
    "            for image, labels in train_bar:\n",
    "                image, labels = image.to(self.device), labels.to(self.device)\n",
    "                prediction = model(image)\n",
    "                loss = criterion(prediction, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                train_bar.set_description(f'Loss {loss.data}')\n",
    "                \n",
    "                if self.log_wandb:\n",
    "                    wandb.log({\"train loss\": loss})\n",
    "\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            val_accuracy = []\n",
    "            for image, labels in tqdm(val_loader):\n",
    "                image, labels = image.to(self.device), labels.to(self.device)\n",
    "                prediction = model(image)\n",
    "                loss = criterion(prediction, labels)\n",
    "                val_losses.append(loss.item())\n",
    "                val_accuracy.extend((torch.argmax(prediction, dim=-1) == labels).cpu().numpy())\n",
    "                \n",
    "\n",
    "\n",
    "            val_loss = np.mean(val_losses)\n",
    "            val_acc = np.mean(val_accuracy)\n",
    "            \n",
    "            if self.log_wandb:\n",
    "                    wandb.log({\"val_loss\": val_loss,\n",
    "                              \"val_acc\": val_acc})\n",
    "            print(f'Validation loss {val_loss} after epoch {epoch}')\n",
    "            print(f'Validation accuracy {val_acc} after epoch {epoch}')\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                self.save_checkpoint(\"./best_checkpoint.pt\", epoch, val_loss)\n",
    "                best_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7606c",
   "metadata": {},
   "source": [
    "Затестим на одном VGG блоке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBaseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vgg = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc =  nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2704, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vgg(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.prediction(x)\n",
    "        return x\n",
    "    \n",
    "model_baseline = ModelBaseline()\n",
    "optimizer = torch.optim.SGD(model_baseline.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion =  nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a08ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model_baseline, optimizer, criterion, trainset, valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf69d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wandb --upgrade --quiet\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "config = dict(model = 'VGG',\n",
    "             learning_rate = 0.001,\n",
    "             data = 'CIFAR10')\n",
    "\n",
    "\n",
    "wandb.init(project=\"sem12\",\n",
    "           notes=\"cnn-vgg\",\n",
    "           config=config)\n",
    "\n",
    "\n",
    "wandb.watch(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b2816a",
   "metadata": {},
   "source": [
    "Восстановим чекпоинт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0419aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('best_checkpoint.pt')\n",
    "model = \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a589f1",
   "metadata": {},
   "source": [
    "### Аугментация изображений \n",
    "\n",
    "С помощью поворотов, отражения, добавления шума, сдвигов и других преобразований, картинка немного меняется, однако сохраняет свою прежнюю метку. С помощью функции Compose можно объединять несколько трансформаций изображения, а потом применять их при чтении датасета. Полный список аугментаций доступен [тут](https://pytorch.org/vision/stable/transforms.html). Изучите его и поэкспериментируйте с различными трансформациями изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a840b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./cifar\", \n",
    "    train=True,                             \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "valset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./cifar\", \n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    trainset, \n",
    "    batch_size=16,\n",
    "    shuffle=True, \n",
    "    num_workers=1\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    valset, \n",
    "    batch_size=16,\n",
    "    shuffle=False, \n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c2268",
   "metadata": {},
   "source": [
    "### Как еще улучшить качество модели?\n",
    "\n",
    "Особенность VGG блока заключается в том, что повышение качества работы сети достигается увеличением числа последовательных блоков. При этом число фильтров в каждом новом блоке в два раза больше, чем в предыдущем. Давайте попробуем объединить 3 VGG блока.\n",
    "\n",
    "\n",
    "#### Dropout\n",
    "\n",
    "https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
    "\n",
    "Если сеть имеет сложную архитектуру, то возможно переобучение - процесс, в котором модель слишком сильно подстраивается под тренировочную выборку и потом дает заниженное качество на тестовой. Для борьбы с этим в нейросетях используют Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e530c4",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://files.ai-pool.com/a/59df0e2cc98add51893f784916195478.png\" alt=\"bn\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb4666",
   "metadata": {},
   "source": [
    "#### BatchNorm\n",
    "\n",
    "Для ускорения и стабилизации обучения добавляют BatchNorm.\n",
    "https://arxiv.org/pdf/1502.03167.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed177cc",
   "metadata": {},
   "source": [
    "<img src=\"https://kratzert.github.io/images/bn_backpass/bn_algorithm.PNG\" alt=\"bn\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c03472",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, 3, padding=1),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, 3, padding=1),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    nn.Conv2d(16, 32, 3, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 32, 3, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    nn.Conv2d(32, 64, 3, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, 3, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64 * 4 * 4, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a3d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion =  nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b7532",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, optimizer, criterion, trainset, valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c41707",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694d6765",
   "metadata": {},
   "source": [
    "### Задание 3 (Домашка). Реализуйте архитектуру с картинки. \n",
    "\n",
    "Детали (типо размера сверток) возьмите из оригинальной статьи. https://arxiv.org/pdf/1409.1556.pdf\n",
    "\n",
    "<img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-2.png\" alt=\"bn\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5343c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d70c98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d65eff5",
   "metadata": {},
   "source": [
    "**Main sources**:\n",
    "\n",
    " - https://github.com/m12sl/dl-hse-2021/blob/main/03-training/seminar.ipynb\n",
    "    \n",
    " - https://github.com/hse-ds/iad-deep-learning/blob/master/2021/seminars/sem03/sem03_task.ipynb\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b394554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
